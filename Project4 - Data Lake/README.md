<h4>Project Summary</h4>
In this project we process json files containg song and event data generated by a music streaming app.
From the song_data directory we process json files and extract song and artists data. From the log_data we process json files
containing events data generated by the streaming app, which we use to determine what users activity was on the app. 
We create spark tables on a distributed system (s3 or hadoop), which will simulate a star schema of a traditional database.
This is more scalable on a distributed system as we are not restricted to resources on a single server to process or store the information.


<h4>How to run the python scripts</h4>
The etl.py script can be run from a terminal if it is run locally, or can be submitted to an emr cluster.

<h4>Files in the project</h4>
dl.cfg
This is the config file that contains the secret and access keys for the IAM user that is used to access the AWS account.

etl.py
This is the file that contains the code that actually processes the data.
This file imports the python and spark libraries required to process the data.

There are 4 functions in the script:

1. create_spark_session()
<h6>Parameters</h6>
This functions does not accept any input parameters.

<h6>Function Purpose</h6>
Creates a new spark session or retrieves an active one if one exists.

2. process_song_data(spark, input_data, output_data)
<h6>Parameters</h6>
spark - Spark Session
input_data - location of the input data
output_data - location of the tables that will be created

<h6>Function Purpose</h6>
Process json files in the song_data directory.
These files contain data used to create the song and artists tables.
Both these tables are stored in parquet format and the data will be overwritten on each new execution.
The songs table is partitioned by year and then artist_id

3. process_log_data(spark, input_data, output_data)
<h6>Parameters</h6>
spark - Spark Session
input_data - location of the input data
output_data - location of the tables that will be created

<h6>Function Purpose</h6>
Processes all the json files in the log data diurectory.
Tables created in this function are the users, time and songplays tables.
This function also has contains a udf which is used to convert the epoch date to a datetime value.
Time table consist a different values extracted from the column populated by the udf function.
Songplays contains the song_id and artist id from the songs table, and the events json files contains the song title and duration which
is sued to join back to the songs table.


4. main()
<h6>Parameters</h6>
This functions does not accept any input parameters.

<h6>Function Purpose</h6>
This is the main function in the script and is used to execute all of the above functions.